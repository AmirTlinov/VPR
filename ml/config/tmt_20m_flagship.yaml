# TMT-20M Flagship Training Config
# Curriculum Learning + Enhanced Architecture + Strict Quality Gates

seed: 42
deterministic: true

model:
  input_dim: 4
  context_size: 32      # Increased from 16 for better pattern learning
  embed_dim: 512
  num_layers: 6
  num_heads: 8
  ffn_dim: 2048
  dropout: 0.15         # Slightly higher for regularization
  num_profiles: 5

data:
  train_path: data/flagship/train.parquet
  val_path: data/flagship/val.parquet
  test_path: data/flagship/test.parquet
  context_size: 32
  num_workers: 8
  pin_memory: true

training:
  batch_size: 1536      # RTX 4070 Ti 12GB - target ~9GB VRAM usage
  epochs: 30            # Fewer epochs with much larger batches
  lr: 0.0006            # Higher LR for larger batches (linear scaling from 0.0002*3)
  weight_decay: 0.01
  warmup_steps: 2000
  max_grad_norm: 0.5    # Tighter clipping
  val_every: 1
  save_every: 5
  patience: 15          # More patience for curriculum
  restart_period: 20    # Cosine restart period

optimizer:
  betas: [0.9, 0.999]
  eps: 0.00000001

# Curriculum Learning Configuration
curriculum:
  promotion_threshold: 0.85   # 85% evasion to advance
  demotion_threshold: 0.60    # 60% evasion to go back
  min_epochs_per_level: 3     # Minimum epochs at each level
  current_level_ratio: 0.70   # 70% current level, 30% previous

# Loss weights (adaptive with curriculum)
loss:
  dpi_weight: 3.0             # Main objective (increases with level)
  similarity_weight: 0.3      # Match target profile
  efficiency_weight: 0.2      # Soft bounds on overhead
  confidence_weight: 0.2      # Calibrate confidence
  detector_weight: 0.5        # Detector-specific losses
  label_smoothing: 0.1        # For confidence calibration

# Strict Quality Gates
quality_gates:
  min_evasion_paranoid: 0.85  # 85% vs Paranoid DPI
  min_evasion_china: 0.90     # 90% vs China GFW
  max_delay_ms: 12.0          # Maximum average delay
  max_padding_ratio: 0.20     # Maximum padding overhead

output:
  dir: models/tmt-20m-flagship
  save_every: 5
  log_every: 100
